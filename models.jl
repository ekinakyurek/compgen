using KnetLayers, Statistics, Plots
import Knet: sumabs2, norm

function interact(e, h; sumout=false)
    y = e .* h
    if sumout
        Î± = softmax(mat(sum(y,dims=1)),dims=1)
        reshape(Î±,1,size(Î±)...)
    else
        softmax(y, dims=(1,2))
    end
end

function attend(ea,  h, W, layer; sumout=false)
    Î±   = interact(ea,W(h); sumout=sumout)
    layer(mat(sum(drop(Î± .* h), dims=2))), Î±
end

function encode(morph, xi, xt=nothing)
    e      = morph.encoder(xi.tokens; batchSizes=xi.batchSizes, hy=true).hidden
    H,B,_  = size(e)
    if xt != nothing
        hs = (morph.encoder(ex.tokens; batchSizes=ex.batchSizes, hy=true).hidden  for ex  in xt)
        h  = reshape(cat1d(hs...), H,4morph.num + 1, B)
        Î¼, Î±u = attend(reshape(morph.WeaÎ¼(e),H,1,B), h, morph.WÎ¼a, morph.WÎ¼; sumout=false)
        logÏƒÂ², Î±Ïƒ = attend(reshape(morph.WeaÏƒ(e),H,1,B), h, morph.WÏƒa,  morph.WÏƒ; sumout=false)
        Î¼, logÏƒÂ², (Î±u=Î±u,Î±Ïƒ=Î±Ïƒ)
    else
        e = reshape(e,H,B)
        Î¼ = morph.WÎ¼(e)
        logÏƒÂ² = morph.WÏƒ(e)
        Î¼, logÏƒÂ², nothing
    end
end

function embed_x_concat_z(embed, x, z; concatz = false)
    if concatz
        vcat(embed(x.tokens),z[:, _batchSizes2ids(x)])
    else
        embed(x.tokens)
    end
end

function decode(morph, z, xi=nothing; bow=4, maxL=20, sampler=sample)
    c = morph.Wdec(z)
    h = tanh.(c)
    concatz = morph.decoder.specs.inputSize == (size(morph.dec_embed.weight,1) + size(z,1))
    if isnothing(xi)
         B      = size(h,2)
         input  = fill!(Vector{Int}(undef,B),bow)
         preds  = zeros(Int,B,maxL)
         for i=1:maxL
            if concatz
                e = vcat(morph.dec_embed(input),z)
            else
                e = morph.dec_embed(input)
            end
            out = morph.decoder(e,h,c; hy=true, cy=true)
            h,c = out.hidden, out.memory
            input  = vec(mapslices(sampler, convert(Array,morph.output(out.y)), dims=1))
            preds[:,i] = input
         end
        return preds
    else
        x = pad_packed_sequence(xi, bow, toend=false)
        e = embed_x_concat_z(morph.dec_embed,x,z; concatz=concatz)
        y = morph.decoder(e, h, c; batchSizes=x.batchSizes).y
        morph.output(drop(y))
    end
end

function decodensample(morph, z, xi; bow=4, maxL=20, sampler=sample)
    nz,nsample,B = size(z)
    z  = reshape(z,nz,nsample * B)
    c = morph.Wdec(z)
    h = tanh.(c)
    x,_ = nsample_packed_sequence(xi, bow, toend=false, nsample=nsample)
    concatz = morph.decoder.specs.inputSize == (size(morph.dec_embed.weight,1) + size(z,1))
    e = embed_x_concat_z(morph.dec_embed,x,z; concatz=concatz)
    y = morph.decoder(e, h, c ; batchSizes=x.batchSizes).y
    morph.output(drop(y))
end

function llsingleseq(lp,yt,inds)
    y = lp[:, inds]
    a = yt.tokens[inds]
    sum(y[findindices(y, a, dims=1)])
end

function ppl_iw(morph, vocab, xi, x; nsample=500)
    Î¼, logÏƒÂ², _ =  encode(morph, xi, isencatt(morph) ? x : nothing)
    nz, B =size(Î¼)
    Î¼ = reshape(Î¼, nz, 1, B)
    logÏƒÂ² =  reshape(logÏƒÂ², nz, 1, B)
    z = Î¼ .+ arrtype(randn(eltype(Î¼),nz, nsample, B)) .* exp.(0.5f0 .* logÏƒÂ²)
    log_pz_qzx =  iws(Î¼, logÏƒÂ², z)
    y  = decodensample(morph, z, xi; bow=vocab.specialIndices.bow)
    yt,inds = nsample_packed_sequence(xi, vocab.specialIndices.eow, nsample=nsample)
    lp = logp(y,dims=1)
    # Calculate log(p(x|z,ðœƒ)) for each nsamples and for each instance
    log_pxz = [llsingleseq(lp,yt,inds[(j-1)*nsample + i])  for i=1:nsample, j=1:B] |> arrtype
    -sum(logsumexp(log_pxz .+  log_pz_qzx, dims=1) .- Float32(log(nsample)))
end

# Calculate log(p(z)/q(z|x,É¸)) for each nsamples and for each instance
function iws(Î¼, logÏƒÂ², z)
    nz,nsamples,B = size(z)
    ÏƒÂ² = exp.(logÏƒÂ²)
    dev = z .- Î¼
    log_qzx = -0.5f0 * sum(dev.^2 ./ ÏƒÂ², dims=1) .- 0.5f0 * (nz * Float32(log(2Ï€)) .+ sum(logÏƒÂ², dims=1))
    log_pz = sum(-0.5f0 * Float32(log(2Ï€)).- z.^2 ./ 2,dims=1)
    reshape(log_pz .- log_qzx,nsamples,B)
end


function calc_ppl(model, data, vocab; nsample=500, B=16)
    edata = Iterators.Stateful(data)
    lss, nchars, ninstances = 0.0, 0, 0
    while (d = getbatch(edata,B)) !== nothing
        J = ppl_iw(model, vocab, d[1], d[2]; nsample=nsample)
        nchars += length(d[1].tokens)
        ninstances += d[1].batchSizes[1]
        lss += J
    end
    exp(lss/nchars)
end

function loss(morph, vocab, xi, x; klw=1.0f0, fbr=nothing)
    Î¼, logÏƒÂ², _ =  encode(morph, xi, isencatt(morph) ? x : nothing)
    z = Î¼ .+ randn!(similar(Î¼)) .* exp.(0.5f0 .* logÏƒÂ²)
    y  = decode(morph, z, xi; bow=vocab.specialIndices.bow)
    KL = 0.5f0 .* (Î¼.^2 .+ exp.(logÏƒÂ²) .- 1.0f0 .- logÏƒÂ²)
    if fbr !== nothing
        s = relu.(KL .- fbr)
        KL = sum((KL .* s) ./ (s .+ Float32(1e-20)))
    else
        KL = sum(KL)
    end
    yt = pad_packed_sequence(xi, vocab.specialIndices.eow)
    B  = size(Î¼,2)
    L  = nllmask(y,yt.tokens; average=false) / B + klw * KL / B
end

function attentions(model, data, vocab;  B=32)
    edata = Iterators.Stateful(data)
    attentions = []
    while ((d = getbatch(edata,B)) !== nothing)
        xi,xt,perms = d
        Î¼,logÏƒÂ²,Î±s = encode(model, xi, xt)
        sfs = map(inds -> xi.tokens[inds],_batchSizes2indices(xi.batchSizes))
        exs = [map(inds -> x.tokens[inds],_batchSizes2indices(x.batchSizes)) for x in xt]
        push!(attentions, (sfs,exs,perms, map(Array,Î±s)))
    end
    return attentions
end

function train_ae!(model, train, vocab; dev=nothing, epoch=30, optim=Adam(), B=16)
    setoptim!(model,optim)
    for i=1:epoch
        lss,cnt = 0.0, 0
        edata = Iterators.Stateful(encode(shuffle(train),vocab))
        while (d = getbatch(edata,B)) !== nothing
            J = @diff loss_ae(model, vocab, d[1], d[2])
            lss += value(J); cnt += 1
            for w in KnetLayers.params(J)
                g = grad(J,w)
                if !isnothing(g)
                    KnetLayers.update!(value(w), g, w.opt)
                end
            end
        end
        if !isnothing(dev)

        else
            println((loss=lss/cnt,))
        end
    end
    return model
end

function loss_ae(morph, vocab, xi, x;)
    z, _ , _ =  encode(morph, xi, isencatt(morph) ? x : nothing)
    y        =  decode(morph, z, xi; bow=vocab.specialIndices.bow)
    yt       =  pad_packed_sequence(xi, vocab.specialIndices.eow)
    B        =  size(z,2)
    L        =  nllmask(y,yt.tokens; average=false) / B
end

function train_vae!(model, train, vocab; dev=nothing, epoch=30, optim=Adam(), B=16, kl_weight=0.0f0, kl_rate = 0.1f0, fb_rate=8.0f0)
    setoptim!(model,optim)
    dim_target_rate = isnothing(fb_rate) ?   nothing :  fb_rate / latentsize(model)
    for i=1:epoch
        lss,cnt = 0.0, 0
        edata = Iterators.Stateful(encode(shuffle(train),vocab))
        while (d = getbatch(edata,B)) !== nothing
            J = @diff loss(model, vocab, d[1], d[2]; klw=kl_weight, fbr = dim_target_rate)
            lss += value(J)
            cnt += 1
            for w in KnetLayers.params(J)
                KnetLayers.update!(value(w), grad(J,w), w.opt)
            end
        end
        kl_weight = Float32(min(1.0, kl_weight+kl_rate))
        if !isnothing(dev)

        else
            println((kl_weight=kl_weight, fbr=fb_rate, loss=lss/cnt))
        end
    end
end

function train_rnnlm!(model, train, vocab; dev=nothing, epoch=30, optim=Adam(), B=16)
    setoptim!(model,optim)
    for i=1:epoch
        lss,cnt = 0.0, 0
        edata = Iterators.Stateful(encode(shuffle(train), vocab))
        while (d = getbatch(edata,B)) !== nothing
            J = @diff losslm(model, vocab, d[1])
            lss += value(J)
            cnt += 1
            for w in KnetLayers.params(J)
                KnetLayers.update!(value(w), grad(J,w), w.opt)
            end
        end
        if !isnothing(dev)

        else
            println((loss=lss/cnt,))
        end
    end
end

function samplingparams(model, data; useprior=false, B=16)
    H,T   = latentsize(model), elementtype(model)
    Î¼, ÏƒÂ² = fill!(arrtype(undef,H,1),0), fill!(arrtype(undef,H,1),1)
    if useprior
        Î¼, ÏƒÂ²
    else
        fill!(ÏƒÂ²,0)
        cnt = 0
        edata = Iterators.Stateful(data)
        while ((d = getbatch(edata,B)) !== nothing)
            Î¼i, logÏƒÂ² = encode(model, d[1], isencatt(model) ? d[2] : nothing)
            Î¼  .+= sum(Î¼i,dims=2)
            ÏƒÂ² .+= sum(exp.(logÏƒÂ²),dims=2)
            cnt += size(Î¼i,2)
        end
        Î¼/cnt, sqrt.(ÏƒÂ²/(cnt-1))
    end
end

function calc_au(model,data; delta=0.01, B=16)
    H, T = latentsize(model), elementtype(model)
    Î¼    = fill!(arrtype(undef, H,1),0)
    cnt  = 0
    edata = Iterators.Stateful(data)
    while ((d = getbatch(edata,B)) !== nothing)
            Î¼i,_ = encode(model, d[1], isencatt(model) ? d[2] : nothing)
            Î¼   .+= sum(Î¼i,dims=2)
            cnt  += size(Î¼i,2)
    end
    Î¼avg =  Î¼/cnt
    cnt=0
    var = fill!(arrtype(undef, H,1),0)
    edata = Iterators.Stateful(data)
    while ((d = getbatch(edata,B)) !== nothing)
        Î¼i, _= encode(model, d[1], isencatt(model) ? d[2] : nothing)
        var  .+= sum((Î¼i .-  Î¼avg).^2, dims=2)
        cnt   += size(Î¼i,2)
    end
    au_var  = convert(Array, var/(cnt-1))
    return sum(au_var .>= delta), au_var,  Î¼avg
end


function calc_mi(model,data; B=16)
    H,T   = latentsize(model), elementtype(model)
    cnt = 0
    mu_batch_list, logvar_batch_list = [], []
    neg_entropy = 0.0
    edata = Iterators.Stateful(data)
    nz=0
    while ((d = getbatch(edata,B)) !== nothing)
        Î¼,logÏƒÂ² = encode(model, d[1], isencatt(model) ? d[2] : nothing)
        nz,B = size(Î¼)
        cnt  += B
        neg_entropy += sum(-0.5f0 * nz * Float32(log(2Ï€)) .- 0.5f0 .* sum(1f0 .+ logÏƒÂ², dims=1))
        push!(mu_batch_list, convert(Array, Î¼))
        push!(logvar_batch_list, convert(Array,logÏƒÂ²))
    end
    neg_entropy = neg_entropy / cnt
    log_qz = 0.0
    mu      =  arrtype(reshape(hcat((mu_batch_list[i] for i in  1:length(mu_batch_list) )...),nz,cnt,1))
    logvar  =  arrtype(reshape(hcat((logvar_batch_list[i] for i in 1:length(logvar_batch_list))...),nz,cnt,1))
    var     =  exp.(logvar)
    cnt2    = 0
    for i=1:length(mu_batch_list)
        Î¼ = arrtype(mu_batch_list[i])
        nz,B = size(Î¼)
        logÏƒÂ² = arrtype(logvar_batch_list[i])
        z_samples = Î¼ .+ randn!(similar(Î¼)) .* exp.(0.5f0 .* logÏƒÂ²)
        cnt2 += B
        z_samples = reshape(z_samples, nz,1,B)
        dev = z_samples .- mu
        log_density = -0.5f0 * sum(dev.^2 ./ var, dims=1) .- 0.5f0 * (nz * Float32(log(2Ï€)) .+ sum(logvar, dims=1))
        log_density = reshape(log_density,cnt,B)
        log_qz += sum(logsumexp(log_density, dims=1) .- log(cnt))
    end
    log_qz /= cnt2
    mi = neg_entropy - log_qz
    return mi
end


function sample(model, vocab, data; N=5, useprior=true)
    Î¼, Ïƒ =  samplingparams(model, data; useprior=useprior)
    samples = []
    for i = 1 : (N Ã· 5) +1
        r     =  similar(Î¼,size(Î¼,1),5)
        z     =  Î¼ .+ randn!(r) .*  Ïƒ
        y     = decode(model, z)
        s     = mapslices(x->trim(x,vocab),y, dims=2)
        push!(samples,s)
    end
    cat1d(samples...)
end


function sampleinter(model, vocab, data; N=5, useprior=true)
    edata = Iterators.Stateful(encode(shuffle(data),vocab))
    d = getbatch(edata,2)
    Î¼, logÏƒÂ² = encode(model, d[1], isencatt(model) ? d[2] : nothing)
    r     =  similar(Î¼,size(Î¼,1),2)
    z     =  Î¼ .+ randn!(r) .* exp.(0.5 .* logÏƒÂ²)
    y     = decode(model, z; sampler=argmax)
    ss,se = mapslices(x->trim(x,vocab),y, dims=2)
    zs,ze = z[:,1],z[:,2]
    delta = (ze-zs) ./ N
    samples = [ss]
    for i=1:N
        zi = zs + i*delta
        y  = decode(model, zi; sampler=argmax)
        si = first(mapslices(x->trim(x,vocab),y, dims=2))
        push!(samples,si)
    end
    push!(samples,se)
    return samples
end
numlayers(model)   = Int(model.decoder.specs.numLayers)
hiddensize(model)  = model.hiddenSize
latentsize(model)  = model.latentSize
elementtype(model) = eltype(value(first(params(model))))
isencatt(model) = haskey(model, :WÎ¼a) &&  haskey(model, :WÏƒa)
function VAE(V, num; H=512, E=16, Z=16, concatz=false, pdrop=0.4)
    encoder      = LSTM(input=V,hidden=H,embed=E)
    dec_embed    = Embed(input=V,output=E)
    decoder      = LSTM(input=(concatz ? E+Z : E),hidden=H,dropout=pdrop)
    transferto!(dec_embed, encoder.embedding)
    return (encoder=encoder,
            WÎ¼=Multiply(input=H, output=Z),
            WÏƒ=Dense(input=H, output=Z, activation=ELU()),
            output=Multiply(input=H,output=V),
            Wdec=Multiply(input=Z, output=H),
            decoder = decoder,
            dec_embed = dec_embed,
            num=num,
            latentSize=Z,
            hiddenSize=H)
end

function EncAttentiveVAE(V, num; H=512, E=16, Z=16, concatz=false, pdrop=0.4)
    encoder    = LSTM(input=V,hidden=H,embed=E)
    dec_embed  = Embed(input=V,output=E)
    decoder    = LSTM(input=(concatz ? E+Z : E),hidden=H,dropout=pdrop)
    transferto!(dec_embed, encoder.embedding)
    return (encoder=encoder,
             WÎ¼=Multiply(input=H, output=Z), #MLP(H, H Ã· 2, Z, activation=ELU()),
             WÏƒ=Dense(input=H, output=Z, activation=ELU()), #MLP(H, H Ã· 2, Z, activation=ELU()),
             WeaÎ¼=Linear(input=H, output=H),
             WeaÏƒ=Linear(input=H, output=H),
             WÎ¼a=Linear(input=H, output=H),
             WÏƒa=Linear(input=H, output=H),
             output=Linear(input=H,output=V),
             Wdec=Multiply(input=Z,output=H),
             decoder = decoder,
             dec_embed = dec_embed,
             num=num,
             latentSize=Z,
             hiddenSize=H)
end

function LSTM_LM(V; H=512, E=16, L=1)
    lstm = LSTM(input=V,hidden=H,embed=E,dropout=0.4,numLayers=L)
    (decoder=lstm, output=Linear(input=H,output=V), hiddenSize=H)
end

function losslm(morph, vocab, xi; average=true)
    y  = decodelm(morph, xi; bow=vocab.specialIndices.bow)
    yt = pad_packed_sequence(xi, vocab.specialIndices.eow)
    nllmask(y,yt.tokens; average=average)
end

# FIXME: Do Multi Layer LM
function decodelm(morph, xi=nothing; bow=4, maxL=20, batch=0, sampler=sample)
    H = hiddensize(morph)
    T = eltype(morph)
    if isnothing(xi)
         B = batch
         h = fill!(arrtype(undef, H, B, numlayers(morph)),0)
         c = fill!(similar(h),0)
         input  = fill!(Vector{Int}(undef,B),bow)
         preds  = zeros(Int, B, maxL)
         for i=1:maxL
            out = morph.decoder(input, h, c; batchSizes=[B], hy=true, cy=true)
            h,c = out.hidden, out.memory
            input  = vec(mapslices(sampler, convert(Array,morph.output(out.y)), dims=1))
            preds[:,i] = input
         end
        return preds
    else
        B = xi.batchSizes[1]
        h = fill!(arrtype(undef, H, B, numlayers(morph)),0)
        c = fill!(similar(h),0)
        x = pad_packed_sequence(xi, bow, toend=false)
        y = morph.decoder(x.tokens, h, c; batchSizes=x.batchSizes).y
        morph.output(drop(y))
    end
end


function calc_ppllm(model, data, vocab; B=16)
    edata = Iterators.Stateful(data)
    lss, nchars, ninstances = 0.0, 0, 0
    while (d = getbatch(edata,B)) !== nothing
        J = losslm(model, vocab, d[1]; average=false)
        nchars += length(d[1].tokens)
        ninstances += d[1].batchSizes[1]
        lss += J
    end
    exp(lss/nchars)
end


function samplelm(model, vocab; N=16, B=16)
    samples = []
    for i = 1 : (N Ã· 16) +1
        y     = decodelm(model; batch=B)
        s     = mapslices(x->trim(x,vocab),y, dims=2)
        push!(samples,s)
    end
    cat1d(samples...)
end

struct Pw{T<:AbstractFloat}
    p::T
    K::T
    a::T
    b::T
    d::T
    beta
end

function Pw{T}(p,K) where T
    dim = p-1
    a = (dim + 2K + sqrt(4K^2 + dim^2))/4
    b = (-2K  + sqrt(4K^2 + dim^2))/dim
    d = 4a*b/(1+b) - dim*log(dim)
    beta = Beta(dim/2, dim/2)
    return Pw(T(p),T(K),T(a),T(b),T(d),beta)
end

function sample(pw::Pw{T}) where T
    opb = (1+pw.b)
    omb = (1-pw.b)
    while true
        Î²  = rand(pw.beta)
        xp = (1-opb*Î²)
        xn = (1-omb*Î²)
        t  = 2*pw.a*pw.b / xn
        if (pw.p-1)*log(t) - t + pw.d - log(rand()) >= 0
            return T(xp / xn)
        end
    end
end



function sample_orthonormal_to(Î¼, p)
    v      = randn!(similar(Î¼))
    r      = sum(Î¼ .* v,dims=1)
    ortho  = v .-  Î¼ .* r
    ortho ./ sqrt.(sumabs2(ortho, dims=1))
end

add_norm_noise(Î¼norm, Ïµ, max_norm=7.5f0) =
     min.(Î¼norm, max_norm-Ïµ) .+ rand!(similar(Î¼norm)) .* Ïµ


function sample_vMF(Î¼, Ïµ, max_norm, pw=Pw{eltype(Î¼)}(size(Î¼,1),10))
    Î¼ = Î¼ .+ Float32(1e-10)
    w = [sample(pw) for i=1:size(Î¼,2)]'
    Î¼norm    = sqrt.(sumabs2(Î¼,dims=1))
    Î¼noise   = add_norm_noise(Î¼norm, Ïµ, max_norm)
    v = sample_orthonormal_to(Î¼ ./ Î¼norm, pw.p)
    scale    = sqrt.(1 .- w.^2)
    Î¼scale = Î¼ .* w ./ Î¼norm
    (v .* scale  + Î¼scale) .* Î¼noise
end


function encode(model, x, xp, ID, pw::Pw; max_norm=7.5f0, Ïµ=0.1f0)
    Î¼ = vcat(model.enclinear(sum(model.embed(ID.I) .* arrtype(ID.Imask),dims=2)),
             model.enclinear(sum(model.embed(ID.D) .* arrtype(ID.Dmask),dims=2)))
    z = sample_vMF(Î¼, Ïµ, max_norm, pw)
end


function decode(model, x, xp, z; max_norm=7.5f0, Ïµ=0.1f0)
    # TODO:
    #bi directional encoder for xp with zero padding: look at macnet
    #encoder with attention # look at macnet
    #look at how to use z in attention or encoder or decoder ?
end
# TODO:
# calculate KL in prototype model
# write loss function for prototype model
# write sampling function for prototype model
# transfer character embeddings from intialization
